{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confirmed-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pv\n",
    "import os\n",
    "import vertica_python\n",
    "from dfmultitool.sources.cdbms import vertica\n",
    "from pyspark.sql import SQLContext, DataFrame, SparkSession\n",
    "import subprocess\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "royal-yemen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password for petrovar@DATA.CORP: \r\n"
     ]
    }
   ],
   "source": [
    "# where ***** input your passport \n",
    "!echo 'Vreulamd1980.' | kinit petrovar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "combined-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jobs name\n",
    "JOB_NAME = \"read_parquet_petrov\"\n",
    "# DYNAMIC_LARGE конфиг\n",
    "spark = SparkSession.builder.appName(JOB_NAME)\\\n",
    ".config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    ".config(\"spark.dynamicAllocation.minExecutors\", \"5\")\\\n",
    ".config(\"spark.dynamicAllocation.maxExecutors\", \"20\")\\\n",
    ".config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    ".config(\"spark.executor.cores\", 5)\\\n",
    ".config('spark.rpc.message.maxSize', '256')\\\n",
    ".config(\"spark.executor.memory\", \"10G\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-prize",
   "metadata": {},
   "source": [
    "### Запись в кафку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jobs name\n",
    "JOB_NAME = \"connection_kafka_2\"\n",
    "# DYNAMIC_LARGE конфиг\n",
    "spark = SparkSession.builder.appName(JOB_NAME)\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"5\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"20\")\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "    .config(\"spark.executor.cores\", 5)\\\n",
    "    .config('spark.rpc.message.maxSize', '256')\\\n",
    "    .config(\"spark.executor.memory\", \"10G\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "file = '/data/raw/fgis_ern/broadcast/2023-05-31/broadcast.1685532297365'\n",
    "logging.info(f\"Путь к файлу hdfs: {file}\")\n",
    "df = spark.read.text(file)\n",
    "\n",
    "TOPIC_NAME='****'\n",
    "SERVER=['****']\n",
    "AUTH_TYPE=\"PLAIN\"\n",
    "username='***'\n",
    "password='***'\n",
    "\n",
    "for row in df.toLocalIterator():\n",
    "    producer = KafkaProducer(\n",
    "            bootstrap_servers=SERVER,\n",
    "            api_version=(0,11,5),\n",
    "            max_block_ms=650000,  # Установить большее значение\n",
    "            request_timeout_ms=300000,\n",
    "            retries=1,\n",
    "            max_request_size=52428800,\n",
    "            sasl_mechanism=AUTH_TYPE,\n",
    "            security_protocol='SASL_PLAINTEXT',\n",
    "            sasl_plain_username=username,\n",
    "            sasl_plain_password=password\n",
    "        )\n",
    "\n",
    "    message = \",\".join(str(field) for field in row)\n",
    "    logging.info(f\"Отправка данных в Kafka: {message.encode('utf-8')}\")\n",
    "    producer.send(TOPIC_NAME, message.encode('utf-8'))\n",
    "\n",
    "    # Закрытие Kafka producer\n",
    "    producer.close()\n",
    "\n",
    "# Закрытие сессии Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-ballet",
   "metadata": {},
   "source": [
    "### Просмотр топиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "actual-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "BOOTSTRAP_SERVERS = ['****']\n",
    "consumer = KafkaConsumer(\n",
    "        bootstrap_servers=BOOTSTRAP_SERVERS,\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        group_id=\"first_consumer\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-london",
   "metadata": {},
   "source": [
    "### Чтение данных из кафки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "laden-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "TOPIC_NAME = \"*****\"\n",
    "BOOTSTRAP_SERVERS = ['***']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consumer = KafkaConsumer(\n",
    "        TOPIC_NAME,\n",
    "        api_version=(0,11,5),\n",
    "        bootstrap_servers=BOOTSTRAP_SERVERS,\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        group_id=\"first_consumer\",\n",
    "    )\n",
    "\n",
    "    print(\"starting the consumer\")\n",
    "    for message in consumer:\n",
    "        print(f\"Message №{i+1}\")\n",
    "        i += 1\n",
    "        print(f\"MESSAGE : {message.value}\", end=\"\\n=====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "weighted-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36-spark-conda",
   "language": "python",
   "name": "python36-spark-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
